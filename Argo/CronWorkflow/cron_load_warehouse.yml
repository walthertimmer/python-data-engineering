apiVersion: argoproj.io/v1alpha1
kind: CronWorkflow
metadata:
  name: cron-load-warehouse
  namespace: argo
  labels:
    app: cron-load-warehouse
    environment: production
  annotations:
    description: "Progress raw to delta warehouse on S3"
spec:
  schedule: "0 0 * * 0"  # Runs every Sunday at midnight
  concurrencyPolicy: Replace
  successfulJobsHistoryLimit: 1
  failedJobsHistoryLimit: 1
  workflowSpec:
    serviceAccountName: argo-workflows-server
    entrypoint: run-script
    templates:
    - name: run-script
      dag:
        tasks:
        - name: load-warehouse-kvk-company
          template: load-warehouse-kvk-company
        - name: load-warehouse-dummy
          template: load-warehouse-dummy
        - name: load-warehouse-rdw-defects
          template: load-warehouse-rdw-defects
        - name: load-warehouse-rdw-defect-definitions  
          template: load-warehouse-rdw-defect-definitions
        - name: load-warehouse-rdw-vehicles
          template: load-warehouse-rdw-vehicles
    - name: notify-status
      steps:
      - - name: send-slack
          template: slack-notification
          arguments:
            parameters:
            - name: status
              value: "{{workflow.status}}"

    - name: load-warehouse-kvk-company
      container:
        image: ghcr.io/walthertimmer/python-data-engineering/python-scripts:latest
        command: ["python", "/scripts/20_load_warehouse.py"]
        imagePullPolicy: Always
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          # limits:
          #   memory: "1Gi" 
          #   cpu: "1"
        env:
          - name: S3_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                name: python-data-engineering
                key: S3_ACCESS_KEY_ID
                namespace: argo
          - name: S3_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                name: python-data-engineering
                key: S3_SECRET_ACCESS_KEY
                namespace: argo
          - name: S3_ENDPOINT_URL
            valueFrom:
              secretKeyRef:
                name: python-data-engineering
                key: S3_ENDPOINT_URL
                namespace: argo
          - name: S3_BUCKET
            valueFrom:
              secretKeyRef:
                name: python-data-engineering
                key: S3_BUCKET
                namespace: argo
          - name: SOURCE_FOLDER
            value: "raw/kvk-company/"
          - name: TARGET_FOLDER
            value: "warehouse"
          - name: FILE_FORMAT
            value: "csv"
          - name: SEPARATOR
            value: ";"
          # - name: ARGO_LOG_LEVEL
          #   value: "debug"
        volumeMounts:
          - name: workdir
            mountPath: /workdir
          - name: ivy-cache
            mountPath: /tmp/.ivy2
        workingDir: /workdir
      volumes:
          - name: workdir
            emptyDir: {}
          - name: ivy-cache
            emptyDir: {}

    - name: load-warehouse-dummy
      container:
        image: ghcr.io/walthertimmer/python-data-engineering/python-scripts:latest
        command: ["python", "/scripts/20_load_warehouse.py"]
        imagePullPolicy: Always
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          # limits:
          #   memory: "1Gi" 
          #   cpu: "1"
        env:
          - name: S3_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                name: python-data-engineering
                key: S3_ACCESS_KEY_ID
                namespace: argo
          - name: S3_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                name: python-data-engineering
                key: S3_SECRET_ACCESS_KEY
                namespace: argo
          - name: S3_ENDPOINT_URL
            valueFrom:
              secretKeyRef:
                name: python-data-engineering
                key: S3_ENDPOINT_URL
                namespace: argo
          - name: S3_BUCKET
            valueFrom:
              secretKeyRef:
                name: python-data-engineering
                key: S3_BUCKET
                namespace: argo
          - name: SOURCE_FOLDER
            value: "raw/dummy/"
          - name: TARGET_FOLDER
            value: "warehouse"
          - name: FILE_FORMAT
            value: "csv"
          - name: SEPARATOR
            value: ";"
        volumeMounts:
          - name: workdir
            mountPath: /workdir
          - name: ivy-cache
            mountPath: /tmp/.ivy2
        workingDir: /workdir
      volumes:
          - name: workdir
            emptyDir: {}
          - name: ivy-cache
            emptyDir: {}

    - name: load-warehouse-rdw-defects
      container:
        image: ghcr.io/walthertimmer/python-data-engineering/python-scripts:latest
        command: ["python", "/scripts/20_load_warehouse.py"]
        imagePullPolicy: Always
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          # limits:
          #   memory: "1Gi" 
          #   cpu: "1"
        env:
          - name: S3_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                name: python-data-engineering
                key: S3_ACCESS_KEY_ID
                namespace: argo
          - name: S3_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                name: python-data-engineering
                key: S3_SECRET_ACCESS_KEY
                namespace: argo
          - name: S3_ENDPOINT_URL
            valueFrom:
              secretKeyRef:
                name: python-data-engineering
                key: S3_ENDPOINT_URL
                namespace: argo
          - name: S3_BUCKET
            valueFrom:
              secretKeyRef:
                name: python-data-engineering
                key: S3_BUCKET
                namespace: argo
          - name: SOURCE_FOLDER
            value: "raw/rdw-defects/"
          - name: TARGET_FOLDER
            value: "warehouse"
          - name: FILE_FORMAT
            value: "csv"
          - name: SEPARATOR
            value: ";"
        volumeMounts:
          - name: workdir
            mountPath: /workdir
          - name: ivy-cache
            mountPath: /tmp/.ivy2
        workingDir: /workdir
      volumes:
          - name: workdir
            emptyDir: {}
          - name: ivy-cache
            emptyDir: {}

    - name: load-warehouse-rdw-defect-definitions
      container:
        image: ghcr.io/walthertimmer/python-data-engineering/python-scripts:latest
        command: ["python", "/scripts/20_load_warehouse.py"]
        imagePullPolicy: Always
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          # limits:
          #   memory: "1Gi" 
          #   cpu: "1"
        env:
          - name: S3_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                name: python-data-engineering
                key: S3_ACCESS_KEY_ID
                namespace: argo
          - name: S3_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                name: python-data-engineering
                key: S3_SECRET_ACCESS_KEY
                namespace: argo
          - name: S3_ENDPOINT_URL
            valueFrom:
              secretKeyRef:
                name: python-data-engineering
                key: S3_ENDPOINT_URL
                namespace: argo
          - name: S3_BUCKET
            valueFrom:
              secretKeyRef:
                name: python-data-engineering
                key: S3_BUCKET
                namespace: argo
          - name: SOURCE_FOLDER
            value: "raw/rdw-defect-definitions/"
          - name: TARGET_FOLDER
            value: "warehouse"
          - name: FILE_FORMAT
            value: "csv"
          - name: SEPARATOR
            value: ";"
        volumeMounts:
          - name: workdir
            mountPath: /workdir
          - name: ivy-cache
            mountPath: /tmp/.ivy2
        workingDir: /workdir
      volumes:
          - name: workdir
            emptyDir: {}
          - name: ivy-cache
            emptyDir: {}

    - name: load-warehouse-rdw-vehicles
      container:
        image: ghcr.io/walthertimmer/python-data-engineering/python-scripts:latest
        command: ["python", "/scripts/20_load_warehouse.py"]
        imagePullPolicy: Always
        resources:
          requests:
            memory: "2Gi"
            cpu: "1000m"
          limits:
            memory: "3Gi" # Set a limit below node capacity
            cpu: "2000m"
        env:
          - name: S3_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                name: python-data-engineering
                key: S3_ACCESS_KEY_ID
                namespace: argo
          - name: S3_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                name: python-data-engineering
                key: S3_SECRET_ACCESS_KEY
                namespace: argo
          - name: S3_ENDPOINT_URL
            valueFrom:
              secretKeyRef:
                name: python-data-engineering
                key: S3_ENDPOINT_URL
                namespace: argo
          - name: S3_BUCKET
            valueFrom:
              secretKeyRef:
                name: python-data-engineering
                key: S3_BUCKET
                namespace: argo
          - name: SOURCE_FOLDER
            value: "raw/rdw-vehicles/"
          - name: TARGET_FOLDER
            value: "warehouse"
          - name: FILE_FORMAT
            value: "json"
        volumeMounts:
          - name: workdir
            mountPath: /workdir
          - name: ivy-cache
            mountPath: /tmp/.ivy2
        workingDir: /workdir
      volumes:
          - name: workdir
            emptyDir: {}
          - name: ivy-cache
            emptyDir: {}

    ## the slack notification
    - name: slack-notification
      inputs:
          parameters:
          - name: status
      container:
        image: curlimages/curl
        command: ["/bin/sh", "-c"]
        args:
        - |
          MESSAGE=$(cat << EOF
          *Workflow Details*
          • Name: {{workflow.name}}
          • Status: {{inputs.parameters.status}}
          • Namespace: {{workflow.namespace}}
          • ID: {{workflow.uid}}
          • Start: {{workflow.creationTimestamp}}
          • Duration: {{workflow.duration}}
          • Node: {{pod.name}}
          EOF
          )
          echo "Sending Slack notification: $MESSAGE"
          curl -X POST -H 'Content-type: application/json' \
          --data "{\"text\":\"$MESSAGE\"}" \
          $(SLACK_WEBHOOK_URL) 2>&1
        env:
        - name: SLACK_WEBHOOK_URL
          valueFrom:
            secretKeyRef:
              name: python-data-engineering
              key: SLACK_WEBHOOK_URL
          
    ## Add onExit handler
    onExit: notify-status

    ## Workflow-level retry strategy
    # retryStrategy:
    #   limit: 1
    #   retryPolicy: OnError
    activeDeadlineSeconds: 3600  # Must complete within 1 hour
    ttlStrategy:
      secondsAfterCompletion: 604800  # 7 days (7 * 24 * 60 * 60)
      successfulWorkflowTTL: "168h"   # 7 days in hours
      failedWorkflowTTL: "168h"       # 7 days in hours
    podGC:
      strategy: OnWorkflowSuccess
      minSuccessfulWorkflowsToRetain: 1  # Always keep at least 1 successful workflow